<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Course Reflection</title>
    <style>
        /* General Styles */
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f4f6f9;
            color: #333;
            line-height: 1.8;
        }
        header {
            background: linear-gradient(135deg, #004080, #0066cc);
            color: #fff;
            text-align: center;
            padding: 30px 10px;
        }
        header h1 {
            margin: 0;
            font-size: 2.2em;
        }
        header p {
            font-size: 1em;
            margin-top: 5px;
        }
        section {
            margin: 40px auto;
            padding: 30px;
            max-width: 900px;
            background-color: #fff;
            border-radius: 10px;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
        }
        h2 {
            font-size: 1.5em;
            margin-bottom: 15px;
            color: #004080;
            border-bottom: 2px solid #e6f7ff;
            padding-bottom: 5px;
        }
        p {
            font-size: 1em;
            margin-bottom: 15px;
        }
        ul {
            margin: 15px 0;
            padding-left: 20px;
        }
        li {
            margin-bottom: 10px;
        }
        strong {
            color: #004080;
        }
        /* Responsive Design */
        @media (max-width: 768px) {
            section {
                margin: 20px;
                padding: 20px;
            }
            header h1 {
                font-size: 1.8em;
            }
            header p {
                font-size: 0.9em;
            }
        }
    </style>
</head>
<body>
    <header>
        <h1>Course Reflection</h1>
        <p>By [Your Name]</p>
    </header>

    <section>
        <h2>Problems in Nature and How to Solve Them</h2>
        <p>When we look at problems in nature, we see patterns that can be approached using methods like iteration, recursion, and backtracking. Iteration involves solving problems step by step in a sequence, which is straightforward and often efficient. Recursion allows us to break a larger problem into smaller, manageable parts, solving each one in a nested manner. Backtracking, which we explored in problems like the N-Queens puzzle, is a systematic way of exploring all possible solutions and rejecting those that don’t meet the criteria.</p>
    </section>

    <section>
        <h2>Time Efficiency and Its Importance</h2>
        <p>In today’s world, time efficiency is often the most crucial factor when solving problems. We focus on how quickly an algorithm can provide a solution, especially in scenarios where delays can have significant consequences, like real-time applications or competitive programming.</p>
        <p>Time efficiencies can range from constant time (Ο(1)), where an algorithm’s execution doesn’t depend on input size, to logarithmic time (Ο(log n)), which is very efficient for large datasets (like binary search). Linear time (Ο(n)) algorithms are practical for many problems, as their performance grows proportionally with input size. Quadratic (Ο(n²)) or cubic (Ο(n³)) time algorithms, however, quickly become impractical as the input size increases.</p>
        <p>Understanding these efficiencies has helped me realize why optimizing for time can be the difference between a functional and an unusable solution in the real world.</p>
    </section>

    <section>
        <h2>Designing Algorithms: What I Learned</h2>
        <p>Through our studies, we explored various design techniques. Divide and conquer stood out as a powerful method. By breaking a problem into smaller subproblems, solving each independently, and combining the results, we can significantly improve efficiency. Merge sort is a perfect example of this approach.</p>
        <p>Traversal methods like depth-first search (DFS) and breadth-first search (BFS) taught us how to navigate data structures like graphs and trees. DFS goes deep along one branch before backtracking, making it ideal for exploring all possibilities, such as finding paths in a maze. BFS explores layer by layer and is often better for finding the shortest path in unweighted graphs.</p>
        <p>We also learned about pruning techniques, such as in the N-Queens problem, where eliminating unnecessary options early can save time. Edge relaxation in tree algorithms helped us simplify shortest path calculations, while level-order traversal and parental dominance clarified hierarchical relationships in data structures like heaps.</p>
    </section>

    <section>
        <h2>Trees and Their Applications</h2>
        <p>Trees are a fundamental data structure that we studied in detail. Binary trees provide an organized way to store data, while binary search trees (BSTs) allow efficient searching. However, BSTs can become unbalanced, so we studied AVL and red-black trees, which automatically balance themselves to maintain performance.</p>
        <p>Heaps taught us about parental dominance, where each parent node has a priority over its children. This structure is especially useful for priority queues. Tries were another fascinating topic. These are special trees optimized for string-based operations like prefix searches. While working with tries, we also explored the Boyer-Moore algorithm, which builds on these ideas for efficient pattern matching by skipping unnecessary comparisons during searches.</p>
    </section>

    <section>
        <h2>Segment Trees and Array Queries</h2>
        <p>We delved into segment trees, which are incredibly efficient for range queries and updates. By dividing an array into segments, these trees allow us to answer queries in logarithmic time. However, updating all segments can sometimes be costly, so we learned about lazy propagation. This technique delays updates until absolutely necessary, improving efficiency.</p>
        <p>Lookup tables, another topic we studied, provide instant access to precomputed results. While faster than segment trees for querying, they require significantly more memory, illustrating the trade-offs we often face in algorithm design.</p>
    </section>

    <section>
        <h2>Comparing Trees and Graphs</h2>
        <p>We also explored the differences between trees and graphs. Trees are hierarchical and have a clear root, making them ideal for problems like representing organizational structures or family trees. Graphs, being more general, can represent complex networks such as road maps or social connections.</p>
        <p>Traversal methods like DFS and BFS apply to both, but their purposes differ. In graphs, DFS is excellent for detecting cycles, while BFS works well for finding shortest paths in unweighted graphs. Trees are particularly effective for hierarchical data and scenarios where relationships are one-to-many.</p>
    </section>

    <section>
        <h2>Sorting Algorithms</h2>
        <p>Sorting algorithms were one of the most detailed topics we explored. Each method has its strengths and weaknesses, and understanding these helped us connect them to real-world applications.</p>
        <ul>
            <li><strong>Bubble Sort:</strong> Repeatedly swaps adjacent elements if they are in the wrong order. Useful mainly for educational purposes or very small inputs.</li>
            <li><strong>Selection Sort:</strong> Finds the smallest element and places it at the start. It’s simple but inefficient for large datasets.</li>
            <li><strong>Insertion Sort:</strong> Builds the sorted list one element at a time, making it efficient for nearly sorted data.</li>
            <li><strong>Merge Sort:</strong> Uses divide and conquer to break the list into halves, sort each, and merge them. Efficient for large datasets.</li>
            <li><strong>Heap Sort:</strong> Uses a binary heap to sort data. It combines efficiency with in-place sorting but isn’t stable.</li>
        </ul>
    </section>

    <section>
        <h2>Graph Algorithms</h2>
        <ul>
            <li><strong>Dijkstra’s Algorithm:</strong> Finds the shortest path in graphs with non-negative weights. Efficient but struggles with negative weights.</li>
            <li><strong>Bellman-Ford Algorithm:</strong> Handles negative weights but is slower, suitable for specific scenarios.</li>
            <li><strong>Floyd-Warshall Algorithm:</strong> Computes shortest paths between all pairs of nodes. Simple but memory-intensive, best for smaller graphs.</li>
            <li><strong>Prim’s Algorithm:</strong> Finds minimum spanning trees efficiently, useful in network design.</li>
        </ul>
    </section>

    <section>
        <h2>What I Learned Overall</h2>
        <p>Overall, I’ve learned that solving problems with algorithms involves making thoughtful trade-offs. We must balance simplicity with efficiency and adapt solutions to meet specific needs. Breaking problems into smaller parts, recognizing patterns, and learning from various contexts have helped me approach challenges effectively. This journey has taught us the importance of planning and adaptability in algorithm design.</p>
    </section>
</body>
</html>
